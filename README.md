
# Fine-Tuning e CriaÃ§Ã£o de Bot com LLMs Locais

Este projeto tem como objetivo realizar fine-tuning em modelos LLM pequenos (como LLaMA3.2 1B e Phi-3-mini).

---

## âš ï¸ Dificuldades Enfrentadas

###  1. LimitaÃ§Ãµes de Hardware Local

- GPU **NVIDIA MX110** - A placa instalada em um notebook nÃ£o Ã© compatÃ­vel com Unsloth para treinamento e nÃ£o Ã© suficiente para outras formas de fine tuning.
- MemÃ³ria VRAM insuficiente para modelos quantizados, causando `ValueError` ao carregar com `transformers`.

###  2. Problemas com InstalaÃ§Ã£o de Modelos

- Bloqueio de acesso via **Zscaler** impediu o download de modelos via `ollama`.

###  3. Tempo de ExecuÃ§Ã£o no Colab

- Ambientes gratuitos como **Google Colab** desconectam por inatividade ou limite de tempo, interrompendo o treinamento.
- Uso de modelos como `LLaMA3.2-1B`, `Mistral` e outros testados, mostrou-se inviÃ¡vel devido ao tempo de carregamento e treinamento, mesmo usando a plataforma Google Colab.

---

## âœ… SoluÃ§Ãµes Aplicadas

###  Uso do modelo `Gwen-3-1B`

- **Mais leve**, rodou razoavelmente bem no Colab T4 (free tier).
- Permitiu fine-tuning com batches pequenos (~500 exemplos) sem erros.

###  Ajustes no CÃ³digo

- **ReducÌ§aÌƒo do tamanho do dataset** para 500 linhas.
- **DivisaÌƒo por batches** e reduÃ§Ã£o no nÃºmero de `steps` e `epochs`.

###  Streamlit + Qwen3

- ApÃ³s o treinamento, o modelo pode ser carregado localmente via **Qwen3** com `ollama`.
- Interface amigÃ¡vel com **Streamlit** para testar perguntas/respostas usando documentos vetorizados (RAG).

---

##  InstruÃ§Ãµes de Uso

###  Requisitos

- Python 3.10+
- CUDA (caso vÃ¡ usar GPU no Colab Pro+)
- Pacotes:
  - `transformers`
  - `unsloth`
  - `trl`
  - `torch`
  - `streamlit`
  - `langchain`
  - `sentence-transformers`
  - `faiss-cpu` ou `chromadb`

```bash
pip install torch transformers trl unsloth streamlit langchain sentence-transformers faiss-cpu
```

###  ExecuÃ§Ã£o Local (com modelo treinado)

```bash
streamlit run app.py
```

---

## ðŸ“¦ Resultado

O modelo nÃ£o performa bem, ele responde algumas perguntas corretamente, mas outras ele alucina ou responde errado. O prazo curto para desenvolvimento foi um agravante mas o resultado ruim deve-se principalmente Ã  restriÃ§Ã£o de hardware.

O Notebook utilizado nÃ£o possuia memÃ³ria RAM ou GPU compatÃ­vel para fine tuning local. Usando o Google Colab foi possÃ­vel fazer o fine tuning mas a versÃ£o free tem uma restriÃ§Ã£o de 24 horas para uso de GPU, alÃ©m do que o Colab desconecta do ambiente se nÃ£o houver atividade na pÃ¡gina, o que forÃ§a a ficar interagindo com a pÃ¡gina atÃ¡ a finalizaÃ§Ã£o do processo.

---

## ðŸ–¼ï¸ Exemplos e Prints

> Coloque aqui prints do terminal, do Colab e da interface do Streamlit.

![alt text](image.png)

![alt text](image-1.png)

![alt text](image-2.png)

---